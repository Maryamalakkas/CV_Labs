{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary TensorFlow modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing TensorFlow Modules and Data Preprocessing\n",
    "This code imports essential TensorFlow modules and performs data preprocessing for training, validation, and testing datasets. The datasets are loaded from directories using tf.keras.preprocessing.image_dataset_from_directory. The training dataset is split into training and validation subsets with a validation split of 20%. The image_size parameter resizes all images to 224x224 pixels, and the batch_size parameter sets the number of samples per batch to 32. A seed is used for reproducibility to ensure consistent data splitting across runs. This setup prepares the datasets for training a machine learning model, ensuring that the images are uniformly sized and batched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2746 files belonging to 5 classes.\n",
      "Using 2197 files for training.\n",
      "Found 2746 files belonging to 5 classes.\n",
      "Using 549 files for validation.\n",
      "Found 924 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary TensorFlow modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Data preprocessing\n",
    "# Load the training dataset from the directory with a validation split\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '/Users/shahadaleissa/CV-labs/Data/archive/train',\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,  # Seed for reproducibility\n",
    "    image_size=(224, 224),  # All images will be resized to 224x224\n",
    "    batch_size=32  # Number of samples per batch\n",
    ")\n",
    "\n",
    "# Load the validation dataset from the same directory\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '/Users/shahadaleissa/CV-labs/Data/archive/train',\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,  # Using the same seed for consistent data splitting\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Load the test dataset from a different directory\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '/Users/shahadaleissa/CV-labs/Data/archive/images',\n",
    "    seed=123,  # Seed for reproducibility\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup Using MobileNetV2\n",
    "This code sets up a neural network model for image classification using a pre-trained MobileNetV2 model from TensorFlow Hub as a feature extractor. The module_url specifies the location of the pre-trained MobileNetV2 model. A tf.keras.Sequential model is constructed with two layers: a hub.KerasLayer that loads the pre-trained model for feature extraction and is set to non-trainable, followed by a dense output layer with a softmax activation function to classify the images into 5 classes. This setup leverages transfer learning to efficiently train a model for a specific task using pre-trained features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model setup\n",
    "# Load a pre-trained MobileNetV2 model from TensorFlow Hub as a feature extractor\n",
    "module_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5\"\n",
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(module_url, trainable=False),  # Feature extraction layer\n",
    "    layers.Dense(5, activation='softmax')  # Output layer with softmax for 5 classes\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Dataset for Performance\n",
    "This code optimizes the training and validation datasets for performance using TensorFlow's data pipeline utilities. The AUTOTUNE parameter enables dynamic tuning of the data loading process. The cache() method caches the dataset in memory to avoid the overhead of re-reading the data from disk for each epoch. The prefetch(buffer_size=AUTOTUNE) method allows data to be preloaded in the background while the model is training, ensuring that data is always available when needed and reducing idle time. These optimizations help to speed up the training process by efficiently managing data loading and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the dataset for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)  # Optimize loading data\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with optimization settings\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 24s 305ms/step - loss: 1.4512 - accuracy: 0.4133 - val_loss: 1.3037 - val_accuracy: 0.4499\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 11s 162ms/step - loss: 1.2104 - accuracy: 0.5448 - val_loss: 1.2001 - val_accuracy: 0.5100\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 11s 160ms/step - loss: 1.1115 - accuracy: 0.5835 - val_loss: 1.1494 - val_accuracy: 0.5228\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 11s 159ms/step - loss: 1.0462 - accuracy: 0.6099 - val_loss: 1.1176 - val_accuracy: 0.5501\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 11s 158ms/step - loss: 0.9964 - accuracy: 0.6309 - val_loss: 1.0957 - val_accuracy: 0.5628\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 11s 158ms/step - loss: 0.9559 - accuracy: 0.6509 - val_loss: 1.0800 - val_accuracy: 0.5774\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 11s 159ms/step - loss: 0.9216 - accuracy: 0.6650 - val_loss: 1.0685 - val_accuracy: 0.5683\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 12s 174ms/step - loss: 0.8917 - accuracy: 0.6850 - val_loss: 1.0599 - val_accuracy: 0.5719\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 11s 163ms/step - loss: 0.8652 - accuracy: 0.6964 - val_loss: 1.0536 - val_accuracy: 0.5792\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 11s 165ms/step - loss: 0.8413 - accuracy: 0.7051 - val_loss: 1.0490 - val_accuracy: 0.5829\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 15  # Adjust the number of epochs as needed\n",
    "history = model.fit(train_dataset, epochs=epochs, validation_data=validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 2s 48ms/step - loss: 8.7687 - accuracy: 0.2673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.768701553344727, 0.2673160135746002]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
